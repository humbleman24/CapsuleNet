import torch
from torch.optim.lr_scheduler import StepLR
from tqdm import tqdm # å¯¼å…¥ tqdm
from torch.utils.tensorboard import SummaryWriter # å¯¼å…¥ SummaryWriter

from capsule import CapsuleNet
from dataset import get_mnist_loaders

# å‡è®¾ device å·²ç»å®šä¹‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ä¿®æ”¹åçš„ train å‡½æ•° ---
def train(model, train_loader, optimizer, criterion, epoch, writer):
    model.train()
    
    # ä½¿ç”¨ tqdm åŒ…è£… train_loaderï¼Œå¹¶æ·»åŠ æè¿°
    loop = tqdm(train_loader, desc=f"Epoch {epoch} [Train]")
    
    for batch_idx, (data, target) in enumerate(loop):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        # --- è®¡ç®—æŒ‡æ ‡ ---
        v_length = torch.sqrt((output**2).sum(dim=2))
        pred = v_length.argmax(dim=1)
        correct = pred.eq(target).sum().item()
        accuracy = correct / target.size(0)
        
        # --- TensorBoard è®°å½• ---
        # è®¡ç®—å…¨å±€æ­¥æ•°ï¼Œç”¨äºåœ¨ TensorBoard ä¸­æ­£ç¡®æ˜¾ç¤º x è½´
        global_step = (epoch - 1) * len(train_loader) + batch_idx
        writer.add_scalar('Loss/train_batch', loss.item(), global_step)
        writer.add_scalar('Accuracy/train_batch', accuracy, global_step)

        # --- æ›´æ–° tqdm è¿›åº¦æ¡çš„åç¼€ä¿¡æ¯ ---
        loop.set_postfix(loss=loss.item(), acc=f"{accuracy:.2%}")

# --- ä¿®æ”¹åçš„ test å‡½æ•° ---
def test(model, test_loader, criterion, epoch, writer):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    # æµ‹è¯•æ—¶ä¹Ÿå¯ä»¥ç”¨ tqdmï¼Œä½†é€šå¸¸ä¿¡æ¯è¾ƒå°‘ï¼Œå¯ä»¥ä¸ç”¨
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            total_loss += criterion(output, target).item()
            
            v_length = torch.sqrt((output**2).sum(dim=2))
            pred = v_length.argmax(dim=1)
            total += target.size(0)
            correct += pred.eq(target).sum().item()

    avg_loss = total_loss / len(test_loader)
    total_accuracy = correct / total

    print(f"\nTest set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{total} ({total_accuracy:.2%})\n")
    
    # --- TensorBoard è®°å½•æ•´ä¸ª epoch çš„æµ‹è¯•ç»“æœ ---
    writer.add_scalar('Loss/test_epoch', avg_loss, epoch)
    writer.add_scalar('Accuracy/test_epoch', total_accuracy, epoch)

    return total_accuracy

# --- ä¿®æ”¹åçš„ main å‡½æ•° ---
def main():
    # 1. åˆå§‹åŒ– SummaryWriterï¼Œæ—¥å¿—ä¼šä¿å­˜åœ¨ 'runs/capsulenet_experiment_1' æ–‡ä»¶å¤¹
    writer = SummaryWriter('runs/capsulenet_experiment_1')

    train_loader, test_loader = get_mnist_loaders(batch_size=128)

    model = CapsuleNet().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = StepLR(optimizer, step_size=5, gamma=0.95)
    criterion = model.margin_loss

    best_accuracy = 0.0
    num_epochs = 50

    for epoch in range(1, num_epochs + 1):
        # å°† writer ä¼ å…¥ train å’Œ test å‡½æ•°
        train(model, train_loader, optimizer, criterion, epoch, writer)
        current_accuracy = test(model, test_loader, criterion, epoch, writer)
        scheduler.step()

        if current_accuracy > best_accuracy:
            best_accuracy = current_accuracy
            print(f"ğŸš€ New best accuracy: {best_accuracy:.2%}. Saving model...")
            torch.save(model.state_dict(), 'best_model.pth')
    
    # 3. è®­ç»ƒç»“æŸåå…³é—­ writer
    writer.close()
    print("Training finished. TensorBoard logs saved.")

if __name__ == '__main__':
    main()